{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model NaiveBayes\n",
      "{'Average FNR': 0.27542881893358373, 'Average FPR': 0.1626086307805345, 'Average OverallError Rate': 0.20735169291709893, 'Average Precision': 0.7829544124633051, 'Average Recall': 0.7809812751429408, 'Average F1 Score': 0.7813316817123709, 'avg roc auc score': 0.7809812751429409}\n",
      "---------------\n",
      "Model Decision Tree\n",
      "{'Average FNR': 0.10234311525347536, 'Average FPR': 0.07457286944629143, 'Average OverallError Rate': 0.0854196925398472, 'Average Precision': 0.909998647576359, 'Average Recall': 0.9115420076501166, 'Average F1 Score': 0.9104841936992896, 'avg roc auc score': 0.9115420076501168}\n",
      "---------------\n",
      "Model SVM\n",
      "{'Average FNR': 0.18819837911937926, 'Average FPR': 0.14191265362778277, 'Average OverallError Rate': 0.1604055456003018, 'Average Precision': 0.8316727525011955, 'Average Recall': 0.8349444836264188, 'Average F1 Score': 0.8328049037613063, 'avg roc auc score': 0.834944483626419}\n",
      "---------------\n",
      "Model Logistic Regression\n",
      "{'Average FNR': 0.1122253633101014, 'Average FPR': 0.0629428056961166, 'Average OverallError Rate': 0.08194143167028199, 'Average Precision': 0.9157160392622903, 'Average Recall': 0.9124159154968912, 'Average F1 Score': 0.9136710279681612, 'avg roc auc score': 0.9124159154968909}\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error, mean_squared_error, precision_recall_fscore_support, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import SpamConstants as sc\n",
    "\n",
    "\n",
    "class ClassifySpam:\n",
    "    \"\"\" Classify Email Spam or Not Spam from UCI Processed Dataset\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Define headers for the dataset file. Initialize Pandas Dataframe to save the results.\"\"\"\n",
    "        self.columns_header_list = []\n",
    "        warnings.filterwarnings(sc.WARNING_IGNORE)\n",
    "        self.res_dataframe = {sc.TRUE_NEGATIVE: [], sc.FALSE_POSITIVE: [],\n",
    "                         sc.FALSE_NEGATIVE: [], sc.TRUE_POSITIVE: [],\n",
    "                         sc.FALSE_POSITIVE_RATE: [], sc.FALSE_NEGATIVE_RATE: [], sc.OVERALL_ERROR_RATE: [],\n",
    "                         sc.MEAN_ABSOLUTE_ERROR: [], sc.MEAN_SQUARED_ERROR: [],\n",
    "                         sc.PRECISION: [], sc.RECALL: [], sc.F1_SCORE: [], sc.ROC_AUC_SCORE: []}\n",
    "\n",
    "        self.stats_dataframe = {sc.AVG_FALSE_NEG_RATE: [], sc.AVG_FALSE_POS_RATE: [],\n",
    "                           sc.AVG_OVERALL_ERROR_RATE: [],\n",
    "                           sc.AVG_PRECISION: [], sc.AVG_RECALL: [], sc.AVG_F1_SCORE: [], sc.AVG_ROC_AUC_SCORE: []}\n",
    "\n",
    "    def createHeaders(self):\n",
    "        \"\"\" Create headers for the UCI dataset file\"\"\"\n",
    "        for i in range(1, sc.WORD_FREQ_COUNT):\n",
    "            self.columns_header_list.append(sc.WORD_FREQUENCY+str(i))\n",
    "        for i in range(1,sc.CHAR_FREQ_COUNT):\n",
    "            self.columns_header_list.append(sc.CHAR_FREQUENCY+str(i))\n",
    "        self.columns_header_list.append(sc.CAP_RUN_LENGTH_AVG)\n",
    "        self.columns_header_list.append(sc.CAP_RUN_LENGTH_LONGEST)\n",
    "        self.columns_header_list.append(sc.CAP_RUN_LENGTH_TOTAL)\n",
    "        self.columns_header_list.append(sc.CLASS_LABEL)\n",
    "\n",
    "    def readInputData(self, filename):\n",
    "        \"\"\" Read the data into Pandas Dataframe\"\"\"\n",
    "        pd_dataframe = pd.read_csv(filename, delimiter=sc.DELIMITER, names = self.columns_header_list)\n",
    "        return pd_dataframe\n",
    "\n",
    "    def createNaiveBayesModel(self, pd_dataframe):\n",
    "        \"\"\" Multinomial Naive Bayes model creation\"\"\"\n",
    "        clf = MultinomialNB()\n",
    "        train_df = pd_dataframe.iloc[:,0:sc.NUM_FEATURES]\n",
    "        train_set = np.array(train_df)\n",
    "        test_set = np.array(pd_dataframe[sc.CLASS_LABEL])\n",
    "        return clf, train_set, test_set, pd_dataframe\n",
    "\n",
    "    def createDTModel(self, pd_dataframe):\n",
    "        \"\"\" Decision Tree classifier model creation\"\"\"\n",
    "        clf = DecisionTreeClassifier()\n",
    "        train_df = pd_dataframe.iloc[:, 0:sc.NUM_FEATURES]\n",
    "        train_set = np.array(train_df)\n",
    "        test_set = np.array(pd_dataframe[sc.CLASS_LABEL])\n",
    "        return clf, train_set, test_set, pd_dataframe\n",
    "\n",
    "    def createSVMModel(self, pd_dataframe):\n",
    "        \"\"\" SVM model creation\"\"\"\n",
    "        clf = SVC(gamma='auto')\n",
    "        train_df = pd_dataframe.iloc[:, 0:sc.NUM_FEATURES]\n",
    "        train_set = np.array(train_df)\n",
    "        test_set = np.array(pd_dataframe[sc.CLASS_LABEL])\n",
    "        return clf, train_set, test_set, pd_dataframe\n",
    "\n",
    "    def createLogReg(self, pd_dataframe):\n",
    "        \"\"\" Logistic Regression model creation\"\"\"\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "        train_df = pd_dataframe.iloc[:, 0:sc.NUM_FEATURES]\n",
    "        train_set = np.array(train_df)\n",
    "        test_set = np.array(pd_dataframe[sc.CLASS_LABEL])\n",
    "        return clf, train_set, test_set, pd_dataframe\n",
    "\n",
    "\n",
    "\n",
    "    def buildKFoldCV(self, clf, train_set, test_set, pd_dataframe):\n",
    "        \"\"\" KFold cross validation with Confusion Matrix\"\"\"\n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        for train_index, test_index in kf.split(pd_dataframe):\n",
    "            X_train, X_test = train_set[train_index], train_set[test_index]\n",
    "            Y_train, Y_test = test_set[train_index], test_set[test_index]\n",
    "\n",
    "            clf.fit(X_train, Y_train)\n",
    "\n",
    "            Y_Predicted = clf.predict(X_test)\n",
    "\n",
    "            roc_auc_score_result = roc_auc_score(Y_test, Y_Predicted)\n",
    "            self.res_dataframe[sc.ROC_AUC_SCORE].append(roc_auc_score_result)\n",
    "\n",
    "            True_Neg, False_Pos, False_Neg, True_Pos = confusion_matrix(Y_test, Y_Predicted).ravel()\n",
    "            self.res_dataframe[sc.TRUE_NEGATIVE].append(True_Neg)\n",
    "            self.res_dataframe[sc.FALSE_POSITIVE].append(False_Pos)\n",
    "            self.res_dataframe[sc.FALSE_NEGATIVE].append(False_Neg)\n",
    "            self.res_dataframe[sc.TRUE_POSITIVE].append(True_Pos)\n",
    "\n",
    "            mean_abs_error = mean_absolute_error(Y_test, Y_Predicted)\n",
    "            self.res_dataframe[sc.MEAN_ABSOLUTE_ERROR].append(mean_abs_error)\n",
    "\n",
    "            mean_sq_error = mean_squared_error(Y_test, Y_Predicted)\n",
    "            self.res_dataframe[sc.MEAN_SQUARED_ERROR].append(mean_sq_error)\n",
    "\n",
    "            precision, recall, f1_score, support = precision_recall_fscore_support(Y_test, Y_Predicted, average='macro')\n",
    "            self.res_dataframe[sc.PRECISION].append(precision)\n",
    "            self.res_dataframe[sc.RECALL].append(recall)\n",
    "            self.res_dataframe[sc.F1_SCORE].append(f1_score)\n",
    "\n",
    "            # FPR = FP/FP+TN\n",
    "            if False_Pos == 0:\n",
    "                false_pos_rate = 0\n",
    "            else:\n",
    "                false_pos_rate = False_Pos/float(False_Pos+True_Neg)\n",
    "            self.res_dataframe[sc.FALSE_POSITIVE_RATE].append(false_pos_rate)\n",
    "\n",
    "            # FNR = FN/FN+TP\n",
    "            if False_Neg == 0:\n",
    "                false_neg_rate = 0\n",
    "            else:\n",
    "                false_neg_rate = False_Neg/float(False_Neg+True_Pos)\n",
    "            self.res_dataframe[sc.FALSE_NEGATIVE_RATE].append(false_neg_rate)\n",
    "\n",
    "            # Overall Misclassification Error= (FP+FN) / (TP+TN+FP+FN)\n",
    "            overall_error_rate = (False_Pos+False_Neg)/(True_Neg + False_Pos + False_Neg + True_Pos)\n",
    "            self.res_dataframe[sc.OVERALL_ERROR_RATE].append(overall_error_rate)\n",
    "        #print(self.res_dataframe)\n",
    "\n",
    "    def evaluateModel(self):\n",
    "        \"\"\" Average of all error rates\"\"\"\n",
    "        avg_fnr = sum(self.res_dataframe[sc.FALSE_NEGATIVE_RATE])/float(len(self.res_dataframe[sc.FALSE_NEGATIVE_RATE]))\n",
    "        self.stats_dataframe[sc.AVG_FALSE_NEG_RATE]=avg_fnr\n",
    "        avg_fpr = sum(self.res_dataframe[sc.FALSE_POSITIVE_RATE])/len(self.res_dataframe[sc.FALSE_POSITIVE_RATE])\n",
    "        self.stats_dataframe[sc.AVG_FALSE_POS_RATE]=avg_fpr\n",
    "        avg_overall_rate = sum(self.res_dataframe[sc.OVERALL_ERROR_RATE])/len(self.res_dataframe[sc.OVERALL_ERROR_RATE])\n",
    "        self.stats_dataframe[sc.AVG_OVERALL_ERROR_RATE]=avg_overall_rate\n",
    "        avg_precision = sum(self.res_dataframe[sc.PRECISION])/len(self.res_dataframe[sc.PRECISION])\n",
    "        self.stats_dataframe[sc.AVG_PRECISION]=avg_precision\n",
    "        avg_recall = sum(self.res_dataframe[sc.RECALL])/len(self.res_dataframe[sc.RECALL])\n",
    "        self.stats_dataframe[sc.AVG_RECALL]=avg_recall\n",
    "        avg_f1_score = sum(self.res_dataframe[sc.F1_SCORE])/len(self.res_dataframe[sc.F1_SCORE])\n",
    "        self.stats_dataframe[sc.AVG_F1_SCORE]=avg_f1_score\n",
    "        avg_score_roc_auc = sum(self.res_dataframe[sc.ROC_AUC_SCORE])/len(self.res_dataframe[sc.ROC_AUC_SCORE])\n",
    "        self.stats_dataframe[sc.AVG_ROC_AUC_SCORE]=avg_score_roc_auc\n",
    "        print(self.stats_dataframe)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifySpamObj = ClassifySpam()\n",
    "    classifySpamObj.createHeaders()\n",
    "    dataset_filename = 'spambase/spambase.data'\n",
    "    models = {'NaiveBayes':classifySpamObj.createNaiveBayesModel,\n",
    "              'Decision Tree':classifySpamObj.createDTModel,\n",
    "              'SVM':classifySpamObj.createSVMModel,\n",
    "              'Logistic Regression':classifySpamObj.createLogReg}\n",
    "\n",
    "    for each_model in models.keys():\n",
    "        classifySpamObj = ClassifySpam()\n",
    "        classifySpamObj.createHeaders()\n",
    "        pd_dataframe = classifySpamObj.readInputData(dataset_filename)\n",
    "        print('Model '+each_model)\n",
    "        model, train_set, test_set, dataframe = models[each_model](pd_dataframe)\n",
    "        classifySpamObj.buildKFoldCV(model, train_set, test_set, dataframe)\n",
    "        classifySpamObj.evaluateModel()\n",
    "        print('---------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
